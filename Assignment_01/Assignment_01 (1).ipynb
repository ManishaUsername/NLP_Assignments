{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Install and Import Libraries"
      ],
      "metadata": {
        "id": "idFAwr3NbdyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gBr2H2tbh0O",
        "outputId": "e3000ea7-578e-4b37-e4d0-13f134053336"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sample Text"
      ],
      "metadata": {
        "id": "roQqnxzTbjZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is amazing! I love NLP, it's very powerful. I'm learning tokenization & stemming.\"\n"
      ],
      "metadata": {
        "id": "z7-NzDghboXE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Whitespace Tokenization"
      ],
      "metadata": {
        "id": "Yi8rM9EAbq4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(wt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_wdrtf2btku",
        "outputId": "9407e906-4661-4bfb-fa54-fd0825901c8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'amazing!', 'I', 'love', 'NLP,', \"it's\", 'very', 'powerful.', \"I'm\", 'learning', 'tokenization', '&', 'stemming.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Punctuation-based Tokenization"
      ],
      "metadata": {
        "id": "sPTcQMqBbxe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "print(wordpunct_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TJzd3iOb0gT",
        "outputId": "5f3ef146-583f-46e5-c82a-fb462427752a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'amazing', '!', 'I', 'love', 'NLP', ',', 'it', \"'\", 's', 'very', 'powerful', '.', 'I', \"'\", 'm', 'learning', 'tokenization', '&', 'stemming', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Treebank Tokenization"
      ],
      "metadata": {
        "id": "2AhLT9dnb3Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "treebank = TreebankWordTokenizer()\n",
        "print(treebank.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFDDhMUFb53W",
        "outputId": "338d44ea-3b07-4584-bcbf-5f522230e285"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'amazing', '!', 'I', 'love', 'NLP', ',', 'it', \"'s\", 'very', 'powerful.', 'I', \"'m\", 'learning', 'tokenization', '&', 'stemming', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Tweet Tokenization"
      ],
      "metadata": {
        "id": "ohkFi9ujclEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet = \"I love #NLP ğŸ˜ Visit https://openai.com @chatgpt\"\n",
        "tt = TweetTokenizer()\n",
        "print(tt.tokenize(tweet))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV8bMzzkb-y3",
        "outputId": "0004d6ac-111d-4643-e2ee-605acba4dafc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', '#NLP', 'ğŸ˜', 'Visit', 'https://openai.com', '@chatgpt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. MWE (Multi-Word Expression) Tokenization"
      ],
      "metadata": {
        "id": "3wV4_Ol1cHV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('New', 'York'), ('machine', 'learning')], separator='_')\n",
        "text2 = \"I live in New York and I study machine learning\"\n",
        "print(mwe.tokenize(text2.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX-eVKFbcJWG",
        "outputId": "eb06ee5b-4e0f-469a-f632-2e2ec065e69a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'live', 'in', 'New_York', 'and', 'I', 'study', 'machine_learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Stemming using Porter Stemmer"
      ],
      "metadata": {
        "id": "UOnO73RBcMNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"easily\", \"fairness\"]\n",
        "\n",
        "for word in words:\n",
        "    print(word, \"->\", ps.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6ReCtPgcO3C",
        "outputId": "c1decec5-2417-469b-8ea1-02d66d0caab3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "runs -> run\n",
            "easily -> easili\n",
            "fairness -> fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Stemming using Snowball Stemmer"
      ],
      "metadata": {
        "id": "fhLVP3DRcRh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "\n",
        "for word in words:\n",
        "    print(word, \"->\", ss.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OWKzKG2cU3S",
        "outputId": "23c6e331-cac3-4dde-b2d9-ee51298ee180"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "runs -> run\n",
            "easily -> easili\n",
            "fairness -> fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Lemmatization using WordNet Lemmatizer"
      ],
      "metadata": {
        "id": "VOmJZkgGcXno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"better\", \"cars\", \"studies\"]\n",
        "\n",
        "for word in words:\n",
        "    print(word, \"->\", lemmatizer.lemmatize(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnSqhEnacccU",
        "outputId": "8f84b1b0-a9e0-492f-b4cf-671b55985481"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> running\n",
            "better -> better\n",
            "cars -> car\n",
            "studies -> study\n"
          ]
        }
      ]
    }
  ]
}